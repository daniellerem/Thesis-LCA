
@article{boeschoten_estimating_2019,
	title = {Estimating the number of serious road injuries per vehicle type in the {Netherlands} by using multiple imputation of latent classes},
	volume = {182},
	issn = {0964-1998, 1467-985X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/rssa.12471},
	doi = {10.1111/rssa.12471},
	abstract = {Statistics that are published by ofﬁcial agencies are often generated by using population registries, which are likely to contain classiﬁcation errors and missing values. A method that simultaneously handles classiﬁcation errors and missing values is multiple imputation of latent classes (MILC). We apply the MILC method to estimate the number of serious road injuries per vehicle type in the Netherlands and to stratify the number of serious road injuries per vehicle type into relevant subgroups by using data from two registries. For this speciﬁc application, the MILC method is extended to handle the large number of missing values in the stratiﬁcation variable ‘region of accident’ and to include more stratiﬁcation covariates. After applying the extended MILC method, a multiply imputed data set is generated that can be used to create statistical ﬁgures in a straightforward manner, and that incorporates uncertainty due to classiﬁcation errors and missing values in the estimate of the total variance.},
	language = {en},
	number = {4},
	urldate = {2021-09-15},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Boeschoten, Laura and Waal, Ton and Vermunt, Jeroen K.},
	month = oct,
	year = {2019},
	pages = {1463--1486},
	file = {Boeschoten e.a. - 2019 - Estimating the number of serious road injuries per.pdf:C\:\\Users\\danie\\Zotero\\storage\\KHYI326J\\Boeschoten e.a. - 2019 - Estimating the number of serious road injuries per.pdf:application/pdf;Boeschoten e.a. - 2019 - Estimating the number of serious road injuries per.pdf:C\:\\Users\\danie\\Zotero\\storage\\IQ2VNWNI\\Boeschoten e.a. - 2019 - Estimating the number of serious road injuries per.pdf:application/pdf;Boeschoten e.a. - 2019 - Estimating the number of serious road injuries per.pdf:C\:\\Users\\danie\\Zotero\\storage\\MNYUNIIF\\Boeschoten e.a. - 2019 - Estimating the number of serious road injuries per.pdf:application/pdf},
}

@article{boeschoten_how_2017,
	title = {How to {Obtain} {Valid} {Inference} under {Unit} {Nonresponse}?},
	volume = {33},
	issn = {2001-7367},
	url = {https://www.sciendo.com/article/10.1515/jos-2017-0045},
	doi = {10.1515/jos-2017-0045},
	abstract = {Abstract
            Weighting methods are commonly used in situations of unit nonresponse with linked register data. However, several arguments in terms of valid inference and practical usability can be made against the use of weighting methods in these situations. Imputation methods such as sample and mass imputation may be suitable alternatives, as they lead to valid inference in situations of item nonresponse and have some practical advantages. In a simulation study, sample and mass imputation were compared to traditional weighting when dealing with unit nonresponse in linked register data. Methods were compared on their bias and coverage in different scenarios. Both, sample and mass imputation, had better coverage than traditional weighting in all scenarios.
            Imputation methods can therefore be recommended over weighting as they also have practical advantages, such as that estimates outside the observed data distribution can be created and that many auxiliary variables can be taken into account. The use of sample or mass imputation depends on the specific data structure.},
	language = {en},
	number = {4},
	urldate = {2021-09-15},
	journal = {Journal of Official Statistics},
	author = {Boeschoten, Laura and Vink, Gerko and Hox, Joop J.C.M.},
	month = dec,
	year = {2017},
	pages = {963--978},
	file = {Boeschoten e.a. - 2017 - How to Obtain Valid Inference under Unit Nonrespon.pdf:C\:\\Users\\danie\\Zotero\\storage\\RJRPSTYQ\\Boeschoten e.a. - 2017 - How to Obtain Valid Inference under Unit Nonrespon.pdf:application/pdf;Boeschoten e.a. - 2017 - How to Obtain Valid Inference under Unit Nonrespon.pdf:C\:\\Users\\danie\\Zotero\\storage\\JDQX6FXJ\\Boeschoten e.a. - 2017 - How to Obtain Valid Inference under Unit Nonrespon.pdf:application/pdf},
}

@article{van_den_bergh_building_2018,
	title = {Building {Latent} {Class} {Growth} {Trees}},
	volume = {25},
	issn = {1070-5511, 1532-8007},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2017.1389610},
	doi = {10.1080/10705511.2017.1389610},
	abstract = {Researchers use latent class growth (LCG) analysis to detect meaningful subpopulations that display different growth curves. However, especially when the number of classes required to obtain a good ﬁt is large, interpretation of the encountered class-speciﬁc curves might not be straightforward. To overcome this problem, we propose an alternative way of performing LCG analysis, which we call LCG tree (LCGT) modeling. For this purpose, a recursive partitioning procedure similar to divisive hierarchical cluster analysis is used: Classes are split until a certain criterion indicates that the ﬁt does not improve. The advantage of the LCGT approach compared to the standard LCG approach is that it gives a clear insight into how the latent classes are formed and how solutions with different numbers of classes relate. The practical use of the approach is illustrated using applications on drug use during adolescence and mood regulation during the day.},
	language = {en},
	number = {3},
	urldate = {2021-09-15},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {van den Bergh, Mattis and Vermunt, Jeroen K.},
	month = may,
	year = {2018},
	pages = {331--342},
	file = {van den Bergh en Vermunt - 2018 - Building Latent Class Growth Trees.pdf:C\:\\Users\\danie\\Zotero\\storage\\T4YZW8ST\\van den Bergh en Vermunt - 2018 - Building Latent Class Growth Trees.pdf:application/pdf},
}

@article{noauthor_notitle_nodate,
}

@article{pankowska_reconciliation_nodate,
	title = {Reconciliation of inconsistent data sources using hidden {Markov} models},
	abstract = {This paper discusses how National Statistical Institutes (NSI’s) can use hidden Markov models (HMMs) to produce consistent ofﬁcial statistics for categorical, longitudinal variables using inconsistent sources. Two main challenges are addressed: ﬁrst, the reconciliation of inconsistent sources with multi-indicator HMMs requires linking the sources on the micro level. Such linkage might lead to bias due to linkage error. Second, applying and estimating HMMs regularly is a complicated and expensive procedure. Therefore, it is preferable to use the error parameter estimates as a correction factor for a number of years. However, this might lead to biased structural estimates if measurement error changes over time or if the data collection process changes. Our results on these issues are highly encouraging and imply that the suggested method is appropriate for NSI’s. Speciﬁcally, linkage error only leads to (substantial) bias in very extreme scenarios. Moreover, measurement error parameters are largely stable over time if no major changes in the data collection process occur. However, when a substantial change in the data collection process occurs, such as a switch from dependent (DI) to independent (INDI) interviewing, re-using measurement error estimates is not advisable.},
	language = {en},
	author = {Pankowska, Paulina and Pavlopoulos, Dimitris and Bakker, Bart and Oberski, Daniel L},
	pages = {20},
	file = {Pankowska e.a. - Reconciliation of inconsistent data sources using .pdf:C\:\\Users\\danie\\Zotero\\storage\\AB9WECMZ\\Pankowska e.a. - Reconciliation of inconsistent data sources using .pdf:application/pdf},
}

@article{li_bayesian_2018,
	title = {Bayesian {Latent} {Class} {Analysis} {Tutorial}},
	volume = {53},
	issn = {0027-3171, 1532-7906},
	url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2018.1428892},
	doi = {10.1080/00273171.2018.1428892},
	abstract = {This article is a how-to guide on Bayesian computation using Gibbs sampling, demonstrated in the context of Latent Class Analysis (LCA). It is written for students in quantitative psychology or related fields who have a working knowledge of Bayes Theorem and conditional probability and have experience in writing computer programs in the statistical language R. The overall goals are to provide an accessible and self-contained tutorial, along with a practical computation tool. We begin with how Bayesian computation is typically described in academic articles. Technical difficulties are addressed by a hypothetical, worked-out example. We show how Bayesian computation can be broken down into a series of simpler calculations, which can then be assembled together to complete a computationally more complex model. The details are described much more explicitly than what is typically available in elementary introductions to Bayesian modeling so that readers are not overwhelmed by the mathematics. Moreover, the provided computer program shows how Bayesian LCA can be implemented with relative ease. The computer program is then applied in a large, real-world data set and explained line-by-line. We outline the general steps in how to extend these considerations to other methodological applications. We conclude with suggestions for further readings.},
	language = {en},
	number = {3},
	urldate = {2021-12-06},
	journal = {Multivariate Behavioral Research},
	author = {Li, Yuelin and Lord-Bessen, Jennifer and Shiyko, Mariya and Loeb, Rebecca},
	month = may,
	year = {2018},
	pages = {430--451},
	file = {Li e.a. - 2018 - Bayesian Latent Class Analysis Tutorial.pdf:C\:\\Users\\danie\\Zotero\\storage\\NK3K3TA3\\Li e.a. - 2018 - Bayesian Latent Class Analysis Tutorial.pdf:application/pdf},
}

@phdthesis{pankowska_measurement_2020,
	title = {Measurement error: estimation, correction, and analysis of implications},
	shorttitle = {Measurement error},
	language = {en},
	author = {Pankowska, Paulina K},
	year = {2020},
	note = {ISBN: 9789464210354
OCLC: 1280053631},
	file = {Pankowska - 2020 - Measurement error estimation, correction, and ana.pdf:C\:\\Users\\danie\\Zotero\\storage\\7LD27ZH2\\Pankowska - 2020 - Measurement error estimation, correction, and ana.pdf:application/pdf},
}

@article{scholtus_editing_nodate,
	title = {Editing and {Estimation} of {Measurement} {Errors} in {Administrative} and {Survey} {Data}},
	language = {en},
	author = {Scholtus, Sander},
	pages = {231},
	file = {Scholtus - Editing and Estimation of Measurement Errors in Ad.pdf:C\:\\Users\\danie\\Zotero\\storage\\9LDZZYIH\\Scholtus - Editing and Estimation of Measurement Errors in Ad.pdf:application/pdf},
}

@article{pavlopoulos_measuring_nodate,
	title = {Measuring temporary employment. {Do} survey or register data tell the truth?},
	abstract = {One of the main variables in the Dutch Labour Force Survey is the variable measuring whether a respondent has a permanent or a temporary job. The aim of our study is to determine the measurement error in this variable by matching the information obtained by the longitudinal part of this survey with unique register data from the Dutch Institute for Employee Insurance. Contrary to previous approaches confronting such datasets, we take into account that also register data are not error-free and that measurement error in these data is likely to be correlated over time. More speciﬁcally, we propose the estimation of the measurement error in these two sources using an extended hidden Markov model with two observed indicators for the type of contract. Our results indicate that none of the two sources should be considered as error-free. For both indicators, we ﬁnd that workers in temporary contracts are often misclassiﬁed as having a permanent contract. Particularly for the register data, we ﬁnd that measurement errors are strongly autocorrelated, as, if made, they tend to repeat themselves. In contrast, when the register is correct, the probability of an error at the next time period is almost zero. Finally, we ﬁnd that temporary contracts are more widespread than the Labour Force Survey suggests, while transition rates between temporary to permanent contracts are much less common than both datasets suggest.},
	language = {en},
	author = {Pavlopoulos, Dimitris and Vermunt, Jeroen K},
	pages = {37},
	file = {Pavlopoulos en Vermunt - Measuring temporary employment. Do survey or regis.pdf:C\:\\Users\\danie\\Zotero\\storage\\R78RM94Q\\Pavlopoulos en Vermunt - Measuring temporary employment. Do survey or regis.pdf:application/pdf;Pavlopoulos en Vermunt - Measuring temporary employment. Do survey or regis.pdf:C\:\\Users\\danie\\Zotero\\storage\\FU45RYYQ\\Pavlopoulos en Vermunt - Measuring temporary employment. Do survey or regis.pdf:application/pdf},
}

@article{zhang_unit-error_nodate,
	title = {A {Unit}-{Error} {Theory} for {Register}-{Based} {Household} {Statistics}},
	language = {en},
	journal = {Journal of Ofﬁcial Statistics},
	author = {Zhang, Li-Chun},
	pages = {18},
	file = {Zhang - A Unit-Error Theory for Register-Based Household S.pdf:C\:\\Users\\danie\\Zotero\\storage\\XJ5BAGTS\\Zhang - A Unit-Error Theory for Register-Based Household S.pdf:application/pdf},
}

@article{zhang_topics_2012,
	title = {Topics of statistical theory for register-based statistics and data integration: \textit{{Developing} theory for data integration}},
	volume = {66},
	issn = {00390402},
	shorttitle = {Topics of statistical theory for register-based statistics and data integration},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9574.2011.00508.x},
	doi = {10.1111/j.1467-9574.2011.00508.x},
	abstract = {Ofﬁcial statistics production based on a combination of data sources, including sample survey, census and administrative registers, is becoming more and more common. Reduction of response burden, gains of production cost efﬁciency as well as potentials for detailed spatial-demographic and longitudinal statistics are some of the major advantages associated with the use of integrated statistical data. Data integration has always been an essential feature associated with the use of administrative register data. But survey and census data should also be integrated, so as to widen their scope and improve the quality. There are many new and difﬁcult challenges here that are beyond the traditional topics of survey sampling and data integration. In this article we consider statistical theory for data integration on a conceptual level. In particular, we present a two-phase life-cycle model for integrated statistical microdata, which provides a framework for the various potential error sources, and outline some concepts and topics for quality assessment beyond the ideal of error-free data. A shared understanding of these issues will hopefully help us to collocate and coordinate efforts in future research and development.},
	language = {en},
	number = {1},
	urldate = {2021-12-06},
	journal = {Statistica Neerlandica},
	author = {Zhang, Li-Chun},
	month = feb,
	year = {2012},
	pages = {41--63},
	file = {Zhang - 2012 - Topics of statistical theory for register-based st.pdf:C\:\\Users\\danie\\Zotero\\storage\\6A6RXHJH\\Zhang - 2012 - Topics of statistical theory for register-based st.pdf:application/pdf},
}

@article{morris_using_2019,
	title = {Using simulation studies to evaluate statistical methods},
	volume = {38},
	issn = {0277-6715, 1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.8086},
	doi = {10.1002/sim.8086},
	language = {en},
	number = {11},
	urldate = {2021-12-06},
	journal = {Statistics in Medicine},
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	month = may,
	year = {2019},
	pages = {2074--2102},
	file = {Morris e.a. - 2019 - Using simulation studies to evaluate statistical m.pdf:C\:\\Users\\danie\\Zotero\\storage\\D2STJV2J\\Morris e.a. - 2019 - Using simulation studies to evaluate statistical m.pdf:application/pdf},
}

@article{linzer_polca_2011,
	title = {\textbf{{poLCA}} : {An} \textit{{R}} {Package} for {Polytomous} {Variable} {Latent} {Class} {Analysis}},
	volume = {42},
	issn = {1548-7660},
	shorttitle = {\textbf{{poLCA}}},
	url = {http://www.jstatsoft.org/v42/i10/},
	doi = {10.18637/jss.v042.i10},
	language = {en},
	number = {10},
	urldate = {2021-12-06},
	journal = {Journal of Statistical Software},
	author = {Linzer, Drew A. and Lewis, Jeffrey B.},
	year = {2011},
	file = {Linzer en Lewis - 2011 - poLCA  An R Package for Polytomous .pdf:C\:\\Users\\danie\\Zotero\\storage\\B7B5SNIW\\Linzer en Lewis - 2011 - poLCA  An R Package for Polytomous .pdf:application/pdf},
}

@incollection{robertson_mixture_2016,
	address = {Cham},
	title = {Mixture {Models}: {Latent} {Profile} and {Latent} {Class} {Analysis}},
	isbn = {978-3-319-26631-2 978-3-319-26633-6},
	shorttitle = {Mixture {Models}},
	url = {http://link.springer.com/10.1007/978-3-319-26633-6_12},
	abstract = {Latent class analysis (LCA) and latent proﬁle analysis (LPA) are techniques that aim to recover hidden groups from observed data. They are similar to clustering techniques but more ﬂexible because they are based on an explicit model of the data, and allow you to account for the fact that the recovered groups are uncertain. LCA and LPA are useful when you want to reduce a large number of continuous (LPA) or categorical (LCA) variables to a few subgroups. They can also help experimenters in situations where the treatment effect is different for different people, but we do not know which people. This chapter explains how LPA and LCA work, what assumptions are behind the techniques, and how you can use R to apply them.},
	language = {en},
	urldate = {2021-12-06},
	booktitle = {Modern {Statistical} {Methods} for {HCI}},
	publisher = {Springer International Publishing},
	author = {Oberski, Daniel},
	editor = {Robertson, Judy and Kaptein, Maurits},
	year = {2016},
	doi = {10.1007/978-3-319-26633-6_12},
	note = {Series Title: Human–Computer Interaction Series},
	pages = {275--287},
	file = {Oberski - 2016 - Mixture Models Latent Profile and Latent Class An.pdf:C\:\\Users\\danie\\Zotero\\storage\\DL9SBBV5\\Oberski - 2016 - Mixture Models Latent Profile and Latent Class An.pdf:application/pdf},
}

@article{kern_tree-based_nodate,
	title = {Tree-based {Machine} {Learning} {Methods} for {Survey} {Research}},
	language = {en},
	author = {Kern, Christoph and Klausch, Thomas and Kreuter, Frauke},
	pages = {21},
	file = {Kern e.a. - Tree-based Machine Learning Methods for Survey Res.pdf:C\:\\Users\\danie\\Zotero\\storage\\M9K5FYGW\\Kern e.a. - Tree-based Machine Learning Methods for Survey Res.pdf:application/pdf},
}

@article{hill_exploring_nodate,
	title = {Exploring {New} {Statistical} {Frontiers} at the {Intersection} of {Survey} {Science} and {Big} {Data}: {Convergence} at “{BigSurv18}”},
	language = {en},
	author = {Hill, Craig A and Biemer, Paul and Buskirk, Trent and Callegaro, Mario and Cazar, Ana Lucía Córdova and Eck, Adam and Japec, Lilli and Kirchner, Antje and Kolenikov, Stas and Lyberg, Lars and Sturgis, Patrick},
	pages = {13},
	file = {Hill e.a. - Exploring New Statistical Frontiers at the Interse.pdf:C\:\\Users\\danie\\Zotero\\storage\\9NUB3BRZ\\Hill e.a. - Exploring New Statistical Frontiers at the Interse.pdf:application/pdf},
}

@article{biemer_estimating_nodate,
	title = {Estimating underreporting of consumer expenditures using {Markov} latent class analysis},
	abstract = {This paper examines reporting in speciﬁc consumer item categories (or commodities) and estimates expenditure underreporting due to survey respondents who erroneously report no expenditure in a category. Our approach for estimating underreporting errors is a two-step process. In the ﬁrst step, a Markov latent class analysis is performed to estimate the proportion of consumers in various subpopulations who fail to report their actual expenditure in a particular commodity. Once this proportion is estimated, the dollar value of the missing expenditure is estimated using the mean expenditure of those in that subpopulation that did report an expenditure. Finally, the estimates are evaluated and discussed in light of external data on expenditure underreporting.},
	language = {en},
	author = {Biemer, Paul P and Tucker, Clyde and Meekins, Brian},
	pages = {13},
	file = {Biemer e.a. - Estimating underreporting of consumer expenditures.pdf:C\:\\Users\\danie\\Zotero\\storage\\ZYL5TJ7K\\Biemer e.a. - Estimating underreporting of consumer expenditures.pdf:application/pdf},
}

@article{biemer_validity_2000,
	title = {On the {Validity} of {Markov} {Latent} {Class} {Analysis} for {Estimating} {Classification} {Error} in {Labor} {Force} {Data}},
	abstract = {The primary goal of this research is to investigate the validity of Markov latent class analysis (MLCA) estimates of labor force classification error and to evaluate the efficacy of MLC analysis as an alternative to traditional methods for evaluating data quality. We analyze interview data from the Current Population Survey (CPS) for the first three months of each of three years – 1993, 1995, and 1996 – and conduct an additional analysis of the CPS unreconciled reinterview data for approximately the same time periods. The reinterview data provides another approach for estimating CPS classification error that, when compared with the MLC estimates, helps to address the validity of the MLCA approach. Five dimensions of MLCA validity are addressed: (a) model diagnostics, (b) model goodness of fit across three years of CPS, (c) agreement between the model and test­retest reinterview estimates of response probabilities, (d) agreement between the model and test­ retest reinterview estimates of inconsistency, and (e) the plausibility of patterns of classification error. In addition, we consider the robustness of the MLCA estimates to violations in the Markov assumption. Our analyses provides no evidence to question the validity of the MLC approach. The method performed well in all five validity tests.},
	language = {en},
	number = {12},
	journal = {Survey Methodology},
	author = {Biemer, Paul P and Bushery, John M},
	year = {2000},
	pages = {14},
	file = {Biemer en Bushery - 2000 - On the Validity of Markov Latent Class Analysis fo.pdf:C\:\\Users\\danie\\Zotero\\storage\\Y2MXYIHW\\Biemer en Bushery - 2000 - On the Validity of Markov Latent Class Analysis fo.pdf:application/pdf},
}

@article{benedetti_tree-based_2005,
	title = {A {Tree}-{Based} {Approach} to {Forming} {Strata} in {Multipurpose} {Business} {Surveys}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=688362},
	doi = {10.2139/ssrn.688362},
	abstract = {The design of a stratified simple random sample without replacement from a finite population deals with two main issues: the definition of a rule to partition the population into strata, and the allocation of sampling units in the selected strata. This article examines a tree-based strategy which plans to approach jointly these issues when the survey is multipurpose and multivariate information, quantitative or qualitative, is available. Strata are formed through a hierarchical divisive algorithm that selects finer and finer partitions by minimizing, at each step, the sample allocation required to achieve the precision levels set for each surveyed variable. In this way, large numbers of constraints can be satisfied without drastically increasing the sample size, and also without discarding variables selected for stratification or diminishing the number of their class intervals. Furthermore, the algorithm tends not to define empty or almost empty strata, thus avoiding the need for strata collapsing aggregations. The procedure was applied to redesign the Italian Farm Structure Survey. The results indicate that the gain in efficiency held using our strategy is nontrivial. For a given sample size, this procedure achieves the required precision by exploiting a number of strata which is usually a very small fraction of the number of strata available when combining all possible classes from any of the covariates.},
	language = {en},
	urldate = {2021-12-06},
	journal = {SSRN Electronic Journal},
	author = {Benedetti, Roberto and Espa, Giuseppe and Lafratta, Giovanni},
	year = {2005},
	file = {Benedetti e.a. - 2005 - A Tree-Based Approach to Forming Strata in Multipu.pdf:C\:\\Users\\danie\\Zotero\\storage\\956GR22K\\Benedetti e.a. - 2005 - A Tree-Based Approach to Forming Strata in Multipu.pdf:application/pdf},
}

@article{linzer_polca_nodate,
	title = {{poLCA}: {Polytomous} {Variable} {Latent} {Class} {Analysis} {Version} 1.4},
	language = {en},
	author = {Linzer, Drew A},
	pages = {31},
	file = {Linzer - poLCA Polytomous Variable Latent Class Analysis V.pdf:C\:\\Users\\danie\\Zotero\\storage\\ZL2996LI\\Linzer - poLCA Polytomous Variable Latent Class Analysis V.pdf:application/pdf},
}

@article{conde_estimating_nodate,
	title = {Estimating the true error rate of classiﬁcation rules built with additional information. {An} application to a cancer trial},
	abstract = {Classiﬁcation rules that incorporate additional information usually present in discrimination problems are receiving certain attention during the last years as they perform better than the usual rules in poor discrimination problems. Ferna´ndez et al (2006) proved that these rules have a lower unconditional misclassiﬁcation probability than the usual Fisher’s rule but they did not consider the estimation of the conditional error probability when a training sample is given (the so-called true error rate) which is a very interesting parameter in practice.},
	language = {en},
	author = {Conde, David and Salvador, Bonifacio and Fernandez, Miguel A},
	pages = {16},
	file = {Conde e.a. - Estimating the true error rate of classiﬁcation ru.pdf:C\:\\Users\\danie\\Zotero\\storage\\IFAQG87Y\\Conde e.a. - Estimating the true error rate of classiﬁcation ru.pdf:application/pdf},
}

@article{ounpraseuth_estimating_2012,
	title = {Estimating misclassification error: a closer look at cross-validation based methods},
	volume = {5},
	issn = {1756-0500},
	shorttitle = {Estimating misclassification error},
	url = {https://bmcresnotes.biomedcentral.com/articles/10.1186/1756-0500-5-656},
	doi = {10.1186/1756-0500-5-656},
	abstract = {Background: To estimate a classifier’s error in predicting future observations, bootstrap methods have been proposed as reduced-variation alternatives to traditional cross-validation (CV) methods based on sampling without replacement. Monte Carlo (MC) simulation studies aimed at estimating the true misclassification error conditional on the training set are commonly used to compare CV methods. We conducted an MC simulation study to compare a new method of bootstrap CV (BCV) to k-fold CV for estimating classification error. Findings: For the low-dimensional conditions simulated, the modest positive bias of k-fold CV contrasted sharply with the substantial negative bias of the new BCV method. This behavior was corroborated using a real-world dataset of prognostic gene-expression profiles in breast cancer patients. Our simulation results demonstrate some extreme characteristics of variance and bias that can occur due to a fault in the design of CV exercises aimed at estimating the true conditional error of a classifier, and that appear not to have been fully appreciated in previous studies. Although CV is a sound practice for estimating a classifier’s generalization error, using CV to estimate the fixed misclassification error of a trained classifier conditional on the training set is problematic. While MC simulation of this estimation exercise can correctly represent the average bias of a classifier, it will overstate the between-run variance of the bias. Conclusions: We recommend k-fold CV over the new BCV method for estimating a classifier’s generalization error. The extreme negative bias of BCV is too high a price to pay for its reduced variance.},
	language = {en},
	number = {1},
	urldate = {2021-12-06},
	journal = {BMC Research Notes},
	author = {Ounpraseuth, Songthip and Lensing, Shelly Y and Spencer, Horace J and Kodell, Ralph L},
	month = dec,
	year = {2012},
	pages = {656},
	file = {Ounpraseuth e.a. - 2012 - Estimating misclassification error a closer look .pdf:C\:\\Users\\danie\\Zotero\\storage\\JZUI7HAU\\Ounpraseuth e.a. - 2012 - Estimating misclassification error a closer look .pdf:application/pdf},
}

@article{van_delden_accuracy_2016,
	title = {Accuracy of {Mixed}-{Source} {Statistics} as {Affected} by {Classification} {Errors}},
	volume = {32},
	issn = {2001-7367},
	url = {https://www.sciendo.com/article/10.1515/jos-2016-0032},
	doi = {10.1515/jos-2016-0032},
	abstract = {Abstract
            Publications in official statistics are increasingly based on a combination of sources. Although combining data sources may result in nearly complete coverage of the target population, the outcomes are not error free. Estimating the effect of nonsampling errors on the accuracy of mixed-source statistics is crucial for decision making, but it is not straightforward. Here we simulate the effect of classification errors on the accuracy of turnover-level estimates in car-trade industries. We combine an audit sample, the dynamics in the business register, and expert knowledge to estimate a transition matrix of classification-error probabilities. Bias and variance of the turnover estimates caused by classification errors are estimated by a bootstrap resampling approach. In addition, we study the extent to which manual selective editing at micro level can improve the accuracy. Our analyses reveal which industries do not meet preset quality criteria. Surprisingly, more selective editing can result in less accurate estimates for specific industries, and a fixed allocation of editing effort over industries is more effective than an allocation in proportion with the accuracy and population size of each industry. We discuss how to develop a practical method that can be implemented in production to estimate the accuracy of register-based estimates.},
	language = {en},
	number = {3},
	urldate = {2021-12-06},
	journal = {Journal of Official Statistics},
	author = {van Delden, Arnout and Scholtus, Sander and Burger, Joep},
	month = sep,
	year = {2016},
	pages = {619--642},
	file = {van Delden e.a. - 2016 - Accuracy of Mixed-Source Statistics as Affected by.pdf:C\:\\Users\\danie\\Zotero\\storage\\4KYTTCJK\\van Delden e.a. - 2016 - Accuracy of Mixed-Source Statistics as Affected by.pdf:application/pdf},
}

@article{scholtus_accuracy_nodate,
	title = {On the accuracy of estimators based on a binary classifier},
	language = {en},
	author = {Scholtus, Sander and van Delden, Arnout},
	pages = {31},
	file = {Scholtus en van Delden - On the accuracy of estimators based on a binary cl.pdf:C\:\\Users\\danie\\Zotero\\storage\\U8ILGQ4F\\Scholtus en van Delden - On the accuracy of estimators based on a binary cl.pdf:application/pdf},
}

@article{pankowska_how_2020,
	title = {How {Linkage} {Error} {Affects} {Hidden} {Markov} {Model} {Estimates}: {A} {Sensitivity} {Analysis}},
	volume = {8},
	issn = {2325-0984, 2325-0992},
	shorttitle = {How {Linkage} {Error} {Affects} {Hidden} {Markov} {Model} {Estimates}},
	url = {https://academic.oup.com/jssam/article/8/3/483/5506582},
	doi = {10.1093/jssam/smz011},
	abstract = {Abstract
            Hidden Markov models (HMMs) are increasingly used to estimate and correct for classification error in categorical, longitudinal data, without the need for a “gold standard,” error-free data source. To accomplish this, HMMs require multiple observations over time on a single indicator and assume that the errors in these indicators are conditionally independent. Unfortunately, this “local independence” assumption is often unrealistic, untestable, and a source of serious bias. Linking independent data sources can solve this problem by making the local independence assumption plausible across sources, while potentially allowing for local dependence within sources. However, record linkage introduces a new problem: the records may be erroneously linked or incorrectly not linked. In this paper, we investigate the effects of linkage error on HMM estimates of transitions between employment contract types. Our data come from linking a labor force survey to administrative employer records; this linkage yields two indicators per time point that are plausibly conditionally independent. Our results indicate that both false-negative and false-positive linkage error turn out to be problematic primarily if the error is large and highly correlated with the dependent variable. Moreover, under certain conditions, false-positive linkage error (mislinkage) in fact acts as another source of misclassification that the HMM can absorb into its error-rate estimates, leaving the latent transition estimates unbiased. In these cases, measurement error modeling already accounts for linkage error. Our results also indicate where these conditions break down and more complex methods would be needed.},
	language = {en},
	number = {3},
	urldate = {2021-12-06},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Pankowska, Paulina and Bakker, Bart F M and Oberski, Daniel L and Pavlopoulos, Dimitris},
	month = jun,
	year = {2020},
	pages = {483--512},
	file = {Pankowska e.a. - 2020 - How Linkage Error Affects Hidden Markov Model Esti.pdf:C\:\\Users\\danie\\Zotero\\storage\\A3UGTTBI\\Pankowska e.a. - 2020 - How Linkage Error Affects Hidden Markov Model Esti.pdf:application/pdf},
}

@article{pankowska_reconciliation_nodate-1,
	title = {Reconciliation of inconsistent data sources by correction for measurement error: the feasibility of parameter re-use},
	abstract = {National Statistical Institutes (NSIs) often obtain information about a single variable from separate data sources. Administrative registers and surveys, in particular, often provide overlapping information on a range of phenomena of interest to official statistics. However, even though the two sources overlap, they both contain measurement error that prevents identical units from yielding identical values. Reconciling such separate data sources and providing accurate statistics, which is an important challenge for NSIs, is typically achieved through macro-integration. In this study we investigate the feasibility of an alternative method based on the application of previously obtained results from a recently introduced extension of the Hidden Markov Model (HMM) to newer data. The method allows a reconciliation of separate error-prone data sources without having to repeat the full HMM analysis, provided the estimated measurement error processes are stable over time. As we find that these processes are indeed stable over time, the proposed method can be used effectively for macrointegration, to reconciliate both first-order statistics – e.g. the size of temporary employment in the Netherlands – and second-order statistics – e.g. the amount of mobility from temporary to permanent employment.},
	language = {en},
	author = {Pankowska, Paulina and Bakker, Bart and Oberski, Daniel L and Pavlopoulos, Dimitris},
	pages = {40},
	file = {Pankowska e.a. - Reconciliation of inconsistent data sources by cor.pdf:C\:\\Users\\danie\\Zotero\\storage\\MPB7CJMT\\Pankowska e.a. - Reconciliation of inconsistent data sources by cor.pdf:application/pdf},
}
